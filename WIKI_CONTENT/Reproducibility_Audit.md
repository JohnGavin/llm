# Reproducibility Audit: Ten Simple Rules

This document maps the **Ten Simple Rules for Reproducible Computational Research** (Sandve et al., 2013) to the specific automated skills and agents implemented in this project.

## Summary
The project achieves **100% coverage** of the core reproducibility rules through a combination of **Nix** (environment), **targets** (pipeline), and **Quarto** (literate programming).

---

## Detailed Mapping

### Rule 1: For every result, keep track of how it was produced
*   **Requirement:** Provenance tracking.
*   **Implementation:** The `targets` package automatically tracks dependencies and hashes of code/data.
*   **Skill:** `targets-runner`, `targets-vignettes`
*   **Evidence:** `_targets.yaml` and `_targets/` store containing build history.

### Rule 2: Avoid manual data manipulation steps
*   **Requirement:** No "Excel munging" or manual edits.
*   **Implementation:** All data transformation is codified in R scripts (`R/tar_plans/*.R`) or DuckDB SQL queries.
*   **Skill:** `data-wrangling-duckdb`, `data-engineering-dbt`
*   **Evidence:** `data-engineer` agent enforces SQL-first transformations.

### Rule 3: Archive the exact versions of all external programs used
*   **Requirement:** Environment reproducibility.
*   **Implementation:** **Nix** pins every single system dependency (R, Quarto, libraries) to a specific commit hash of `nixpkgs`.
*   **Skill:** `nix-rix-r-environment`
*   **Evidence:** `default.nix` generated by `rix` contains explicit commit hashes.

### Rule 4: Version control all custom scripts
*   **Requirement:** History of code changes.
*   **Implementation:** Mandatory "9-Step Workflow" requires Git for all changes.
*   **Skill:** `r-package-workflow`, `ci-workflows-github-actions`
*   **Evidence:** `AGENTS.md` mandates `usethis::pr_init` and `gert` usage.

### Rule 5: Record all intermediate results in standardized formats
*   **Requirement:** Persistence of steps.
*   **Implementation:** `targets` stores intermediate objects. Large data uses Parquet/Arrow/DuckDB.
*   **Skill:** `data-wrangling-duckdb`
*   **Evidence:** `inst/extdata/` contains `.json` and `.duckdb` files, not just final plots.

### Rule 6: For analyses that include randomness, note underlying random seeds
*   **Requirement:** Deterministic execution.
*   **Implementation:** `targets` manages RNG seeds for each target individually.
*   **Skill:** `targets-runner`
*   **Evidence:** `_targets.yaml` configuration handles seed generation.

### Rule 7: Always store raw data behind plots
*   **Requirement:** Figures must be data-driven and inspectable.
*   **Implementation:** 
    *   **No Inline Code:** Plots are generated as `tar_target()` artifacts, never via inline `ggplot()` calls in vignettes.
    *   **Hidden Tables:** Every plot is immediately followed by a code-folded `DT::datatable` containing the source data.
*   **Skill:** `reproducible-visualization`, `quarto-dashboards`
*   **Evidence:** `vignettes/*.qmd` use `tar_read()` for all visuals.

### Rule 8: Generate hierarchical analysis output
*   **Requirement:** Layers of detail ordered logically.
*   **Implementation:** 
    *   **Logical Ordering:** Tabs and pages follow factor levels (e.g., "North", "South") or chronological order, not arbitrary or alphabetical ordering.
    *   **Structure:** Summary -> Detail -> Diagnostics.
*   **Skill:** `reproducible-visualization`, `quarto-websites`
*   **Evidence:** The `docs/` folder structure and navigation menus mirror this hierarchy.

### Rule 9: Connect textual statements to underlying results
*   **Requirement:** Literate programming with dynamic values.
*   **Implementation:** 
    *   **Dynamic Text:** All numbers in text use inline code (`` `r max(data)` ``) sourced from `targets`.
    *   **Descriptive Captions:** Captions summarize the plot contents (axes, legend) and list the "Top 5" results.
*   **Skill:** `reproducible-visualization`, `readme-qmd-standard`
*   **Evidence:** `README.qmd` generates `README.md`, ensuring examples are always executable.

### Rule 10: Provide public access to scripts, runs, and results
*   **Requirement:** Open access.
*   **Implementation:** GitHub Repository + GitHub Pages deployment.
*   **Skill:** `pkgdown-deployment`, `shinylive-deployment`
*   **Evidence:** Public URL `johngavin.github.io/llm` hosts the full output.

---

## Conclusion
This project treats reproducibility not as an afterthought but as an **architectural constraint** enforced by the `nix-env`, `targets-runner`, and `data-quality-guardian` agents.